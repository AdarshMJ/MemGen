data: 
  data: node500
  batch_size: 8  # Smaller batch size due to large graphs (500 nodes each)
  max_node_num: 512  # Maximum 500 nodes, using 512 for padding
  feat:
    type: 
      - eig1
      - eig2
    scale: 10
    norm: True
  perm_mix: True

mix:
  x: 
    type: OU
    drift_coeff: 1
    sigma_0: 0.2
    sigma_1: 0.1
    num_scales: 1000
  adj: 
    type: OU
    drift_coeff: 1 
    sigma_0: 0.4
    sigma_1: 0.2
    num_scales: 1000

model:
  type: transformer
  num_layers: 8
  input_dims:
    E: 2
    y: 0
  hidden_mlp_dims:
    X: 128
    E: 64
    y: 128
  hidden_dims:
    dx: 256
    de: 64
    dy: 64
    n_head: 8
    dim_ffX: 256
    dim_ffE: 64
    dim_ffy: 256
  
train:
  name: node500_train
  use_tensorboard: True
  lambda_train: 5
  num_epochs: 10000  # Reduced for initial testing
  save_interval: 500  # Save more frequently
  reduce_mean: False 
  lr: 2.0e-4 
  lr_schedule: False 
  ema: 0.999
  weight_decay: 1.0e-12 
  grad_norm: 1.0
  lr_decay: 0.999
  eps: 2.0e-3 
  optimizer: adamw
  loss_type:
    x: const
    adj: default

sampler:
  predictor: Euler
  corrector: None
  snr: 0.0
  scale_eps: 0.0
  n_steps: 1

sample:
  batch_size: 10  # Smaller batch for sampling large graphs
  use_ema: True 
  eps: 2.0e-3 
  kernel: tv
  noise_removal: True
  seed: 42

ckpt: node500
